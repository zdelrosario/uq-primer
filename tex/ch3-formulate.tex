\documentclass[../primer.tex]{subfiles}

\begin{document}

\chapter{Formulate}
%% --------------------------------------------------

\section{Data}
%% --------------------------------------------------
Data are our quantitative anchor to reality.\footnote{One could
  \href{http://phdcomics.com/comics.php?f=1816}{make a case} for data in the
  plural (data are facts) or in the singular (data is information). We will use
  the former.} They allow us to make numerical statements about the physical
world, and thus are invaluable to solving problems. However, data are not
infallible -- one must have both skepticism and appreciation of variability in
order to use data effectively. We will consider two kinds of data in this
primer:

\textbf{Physical Data} are comprised of measurements of the physical world. The
vast majority of physical measurements involve the comparison of some physical
quantity of interest against a defined standard; this is most obviously seen in
simple measurements of length, where one can compare a meter stick against a
physical object. More complicated measurements involve some quantity which
cannot be directly measured, but can instead be obtained through a
transformation based on a physical model. An example: It would be challenging to
measure pressure directly. However, we can build a barometer out of a sealed
body with fluid, which essentially converts pressure changes into length
changes. Carrying out the transformation from measured lengths to pressures
requires an appeal to hydrostatics, which carries with it some particular
assumptions.

Variation in physical data arises from \emph{unknown variables}, what we
sometimes call \emph{lurking variables}.\cite{box1966} In the barometer example
above, suppose that our instrument experienced both temperature and pressure
changes. If our barometer used a fluid whose density changed with the pressure,
\emph{and if we did not account for the temperature changes}, then even at a
constant external pressure, we might measure fluctuating values. In this case,
we would call temperature a lurking variable. Detecting and controlling lurking
variables is challenging, but necessary to improve physical
measurements.\cite{joiner1981,delRosario2017lurking}

Luckily, the variations in physical data tend to exhibit `nice' properties, and
so often lend themselves to statistical characterization. For this reason, the
tools of probability and statistics are very well-suited to tackling physical
data.

\textbf{Simulation Data} are the result of models. They are connected to reality
only insofar as their generating model is connected to reality. However,
simulation data has a huge advantage over physical data; one can use simulation
data to make quantitative statements about reality \emph{without physical
  testing}. This can be useful for overcoming constraints of cost (as with
aircraft testing) or legality (as with nuclear weapons). Perhaps one of the
greatest triumphs of simulation is human spaceflight. Using little more than
Newton's laws, early pioneers of spaceflight managed to compute trajectories
that sent astronauts to the moon and back -- physical data alone could not have
stood up this effort.

Of course, we should not oversell the value of simulation. The Apollo program
was built on top of the experience and physical data gathered from Project
Gemini, and before that Mercury. Models themselves are built from studying
physical data, and always carry some assumptions -- assumptions which may be
disconnected from reality. While physical data are subject to variability,
models are often subject to \emph{discrepancy}, often modeled as a difference
between a model prediction and the 'true' value that would arise from a perfect
measurement in physical reality.\cite{higdon2004calibration-prediction}

Despite the fundamental difference between variability and discrepancy,
discrepancy is often modeled using probability and statistics as
well.\cite{kennedy2001bayesian,higdon2004calibration-prediction} As I am writing
this sentence, there is an open debate about the suitability of this approach.
Rather than wade into this argument, we will accept the use of probability for
discrepancy as normative, and proceed to treat both physical and simulation data
with the same toolkit.

For what follows, we will consider a set of data $X_i\in\R{}$ for $i =
1,\dots,n$. These data are measurements of some true underlying value $X_i = Y +
\epsilon_i$, where the $\epsilon_i$ are errors arising either from lurking
variables or discrepancies.

\subsection{Summaries}
%% -------------------------
\marginnote{These summary values are more formally called \emph{summary
    statistics}. Since we are describing data here, really we should call the
  following \emph{sample} estimates, in contrast with population values. We will
  make this distinction below when discussing distributions.}

Working directly with data is challenging. Often, there is simply too much
information to make sense of manually. To that end, we often employ
\emph{summaries} -- single numbers which describe a set of data. There are
different kinds of summaries, which serve different purposes. We will introduce
different features of data we might seek to represent, and some summaries that
support them.

\marginnote{\underline{Concept:} \emph{Central tendency} is a notion of where a
  distribution or data are located.}

\textbf{Central tendency} is a key concept with a rather descriptive name. A
measure of central tendency is a single number which describes the `location' or
`typical value' of data. We will discuss two important measures of central
tendency: the \emph{mean} and the \emph{median}.

The \emph{mean}\marginnote{\underline{Definition:} The \emph{sample mean} is the
  sum of all the data in a set, divided by the number of samples. It is a common
  measure of central tendency.} is defined via

\begin{equation} \label{eq:def-sample-mean}
  \overline{X} = \frac{1}{n}\sum_{i=1}^n X_i.
\end{equation}

\noindent In terms of the true value, the mean is given by $\overline{X} = Y +
\frac{1}{n}\sum_{i=1}^n \epsilon_i$. While the mean has a number of useful
statistical properties, it has an intuitive justification: In the case where the
errors $\epsilon_i$ are `evenly' distributed about zero, they will cancel to
yield the true value $Y$.\footnote{We will make this more precise below.}

Figure \ref{fig:michelson-mean} depicts an example usage of the mean; it
presents data that Albert Michelson collected in 1879 to determine the speed of
light.\cite{dorsey1944velocity} There, we use the mean to pursue a `nominal'
value for the speed of light, and compare it against the true value.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.85\textwidth]{./images/michelson_scatter}

  \caption{Albert Michelson's 1880 speed-of-light measurements. The mean is
    appropriate as a nominal value. The exact value is plotted for comparison.
    Visually, Michelson's mean value is `close' to the true value, though this
    begs the question `how close is close?' We can make the notion of
    `closeness' more precise by considering the variability of the data.}
  \label{fig:michelson-mean}
\end{figure}

\marginnote{Somewhat cheekily, we can note that $c = 299,792,458 m/s$ is
  \emph{exact}, as the meter is \emph{defined} in terms of the speed of
  light.\cite{thompson2008}}

While the mean is a sensible measure of central tendency, it does have some
weaknesses. We will illustrate these by contrasting against another measure of
central tendency.

\bigskip
The \emph{median}\marginnote{\underline{Definition:} The \emph{sample median} is
  the middle value of a dataset. It is a robust measure of central tendency.} is
the `middle' value of a dataset, most easily described through an example.
Suppose our data is given by $\{5, 3, 6, -1, 4\}$. We find the median by
ordering the data $\{-1, 3, 4, 5, 6\}$ and selecting the middle number; in this
case $4$. In the case where we have an even number of data points, we average
the middle two values.\footnote{e.g. $\{5, 3, 6, -1\}$ yields $\f12(3+5) = 4$}

\marginnote{Perhaps unsurprisingly, there is no universal agreement on a
  definition for the sample median. We will adopt Tukey's
  definition,\cite{tukey1977exploratory} but know that there are others.}

More generally, we can understand the median in terms of \emph{order
  statistics}, which are simply the ordered values of a dataset. Given our data
$X_i$, the order statistics are defined by

\begin{equation} \label{eq:def-order}
  X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n-1)} \leq X_{(n)}.
\end{equation}

\noindent The median is then given by $X_{(n/2)}$, which is simply notation for
the middle value. A definition which automatically handles the $n$ odd case is
given by

\begin{equation} \label{eq:def-sample-median}
  \tilde{X} = \f12\left(X_{(\floor{n/2})} + X_{(\ceil{n/2})}\right),
\end{equation}

\noindent where $\floor{\cdot}, \ceil{\cdot}$ define the floor and ceiling
functions. Figure \ref{fig:income-median} provides an example usage of the
median using data from the US Census Bureau. A small number of very large values
(sometimes called \emph{outliers}) can significantly impact the mean. In these
cases, the median can be a better measure of central tendency.

This being said, the mean is a more \emph{efficient} (less variable) estimator
of central tendency than the median, under particular
conditions.\cite{mosteller1977data} As we'll recommend below, one should use
multiple techniques in practice, at least when investigating a new set of data.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.85\textwidth]{./images/income}

  \caption{Distribution of sampled household incomes in the US, circa 2016.
    We'll cover density plots a bit later; but for now note that a higher
    density corresponds to a larger number of instances of that income. Notice
    that mean is pulled considerably towards higher incomes, despite the
    preponderance of lower incomes. The median is less affected by a small
    number of large values -- it is often used as a nominal value for income.
    The fact that the mean and median do \emph{not} agree here is telling -- it
    signals that something worth noting is happening in the data.}
  \label{fig:income-median}
\end{figure}

\marginnote{\underline{Concept:} \emph{Spread} is a notion of how disperse a
  distribution or data are.}

\textbf{Spread} is another apt term for a property of data. The spread is
synonymous with the variability of the data. If our data are thought to be
corrupted measurements of a true value, then spread can be thought of as the
inverse of precision. Similar to central tendency, we will introduce two notions
of spread.\footnote{In fact, in statistics lingo, \emph{precision} is inverse
  variance.}

The \emph{standard deviation}\marginnote{\underline{Definition:} The \emph{sample
    variance} is the mean-squared distance of the data from its mean. The
  \emph{sample standard deviation} is its square root. Both are measures of
  spread.} is a common measure of spread. It is defined via

\begin{equation} \label{eq:def-sample-sd}
  S = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X})^2}.
\end{equation}

Note that we must first compute the sample mean $\overline{X}$ before we can
compute $S$. Somewhat mysteriously, the denominator for $S$ is $n-1$, not $n$.
This is a subtle point that will have to wait until we introduce distributions.
For now, it will suffice to say that for small $n$, this factor\footnote{called
  the \emph{Bessel correction}} is necessary to ensure an accurate
estimate.\footnote{A bit more detail: Using the sample mean $\overline{X}$
  within $S$ causes us to loose a \emph{degree of freedom} -- a notion of
  information available from the data. We account for this `double use' of
  information through the Bessel correction.}

One may use the standard deviation to construct ranges, rather than single
values.\marginnote{A wider interval is less trustworthy than a narrow one; there
  are more true values that are compatible with the available data. However, if
  the entire interval is compatible with some conclusion -- if the interval
  itself falls within some acceptable range -- then this enables one to draw a
  more robust conclusion.} A common construction is a \emph{confidence
  interval}, which is a simple idea with some subtle points. Essentially, if
there exists a true value -- like $Y$ -- then a confidence interval will include
that value at some known \emph{confidence level}, often $95\%$ or $99\%$. An
interval gives more information than a mean alone; in addition to location, it
conveys the quality of an estimate, based on its width.

Figure \ref{fig:michelson-ci} depicts a $95\%$ confidence interval for the speed
of light, based on Michelson's 1879 data. Note that this interval includes the
true value, and gives an accurate sense of the spread of the data.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.85\textwidth]{./images/michelson_ci}

  \caption{Albert Michelson's 1880 speed-of-light measurements. The standard
    deviation is used to construct a confidence interval around the sample mean,
    which here includes the true speed of light. We will introduce the notion of
    \emph{confidence intervals} later when we discuss distributions.}
  \label{fig:michelson-ci}
\end{figure}

The \emph{interquartile range}\marginnote{\underline{Definition:} The
  \emph{sample interquartile range} is the distance between the sample
  quartiles. It a robust measure of spread.} (iqr) is to the standard deviation
as the median is to the mean. It is a robust measure of spread, and as its name
implies it involes the difference between \emph{quartiles} (the
quarter-quantiles) of the data. We can compute the interquartile range via

\begin{equation} \label{eq:iqr-sample}
  \text{iqr} = Q_3 - Q_1,
\end{equation}

\noindent where $Q_1$ is the median of the lower $\floor{N/2}$ of the data, and
$Q_3$ is the median of the upper fraction.\cite{tukey1977eda} The mean was
susceptible to outliers; since the variance is essentially a mean of
squared-differences, the standard deviation is similarly vulnerable to outliers.

Using the quartiles and the iqr, we can construct a \emph{boxplot}; a very
useful visual depiction of summaries. The box itself is constructed from the
quartiles of the given data ($Q_1$, the median $Q_2$, and $Q_3$), while the
lines\footnote{sometimes called \emph{whiskers} or \emph{fences}} are extended
from the quartiles by a multiple of the iqr. Points which lie outside these
extended ranges are depicted as points, and are potentially of interest.

While there are multiple versions of the boxplot,\cite{frigge1989some} all are
useful for summarizing data.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.85\textwidth]{./images/michelson_boxplot}

  \caption{Albert Michelson's 1880 speed-of-light measurements. The data are
    presented as a boxplot, which highlights a number of potential outliers.}
\end{figure}

\textcolor{red}{TODO} \textbf{Correlation} is important for studying the
relationship between variables.

\textbf{Skepticism} is vitally important when analyzing data. In practice, one
should be skeptical both of the data, \emph{and} of the techniques used to
analyze the data.

\emph{Techniques} are vulnerable to faulty assumptions -- when analyzing data,
one should employ multiple methods and compare their results.
Tukey\cite{tukey1979robust} stated this quite directly when he wrote

\begin{quote}
It is perfectly proper to use both classical and robust/resistant methods
routinely, and only worry when they differ enough to matter. BUT when they
differ, you should think HARD.
\end{quote}

This is why we covered both classical (mean, standard deviation) and robust
(median, iqr) summaries -- in practice, you should use both, at least when first
studying a new dataset. This will enable you to catch features of the data that
might otherwise go undetected, such as the difference between the mean and
median in the Census data above.

Data are also vulnerable to errors, and may contain outliers. When first
studying a new dataset, one should first check for these sorts of problems.
Common approaches include checking for impossible values (e.g. zero-width
measurements of a diamond) or gross outliers (for which a boxplot is very
helpful).

More broadly, one should engage in \emph{exploratory data analysis} (EDA) before
proceeding with more analytical investigations.\cite{tukey1977eda} This is a
crucial step where one seeks to understand the data, especially to check
assumptions that subsequent analyses may rest upon. However, EDA is a broad
subject, beyond the scope of this primer.

\subsection{Distributions}
%% -------------------------
We will use distributions to model uncertain quantities. A \emph{distribution}
defines a \emph{random variable} $X$ -- an uncertain quantity which takes
(potentially) different values upon different observations. Some examples of
physical phenomena we might model with a random variable include: the outcome of
a coin flip, the roll of a twenty-sided die, the ambient temperature in New York
City, the thrust output of a jet engine subject to uncertain inlet conditions.
Note that some of the preceding examples are discrete (the coin and die), while
the others are continuous (temperature and thrust). While discrete random
variables certainly have their applications, they are less common in scientific
and engineering applications, so we will focus on continuous distributions.

Why would we bother modeling uncertain quantities? In cases where we have data,
we can use a model to do \emph{inference}. Suppose our random variable model
arises from a physical argument, and its fitted parameters have some physical
meaning. In this case, fitting the model is an exercise in learning physical
properties about the process which generated the data, much like the barometer
example above. We will introduce a simple way to fit distributions below.

In cases where we have no data, but can make informed statements about what the
data \emph{might} look like were we to observe them, we can use distributions to
make these statements quantitative.

Formally, a distribution is a \emph{function} which describes how likely a
random variable is to take a particular value. Figure \ref{fig:dist-norm}
provides an illustrative example through displaying the standard normal's
\emph{probability density function} (PDF) and \emph{cumulative density function}
(CDF). Both functions convey the same information, but in different forms; the
PDF can be interpreted as describing the \emph{likelihood} of a single value
$x$, while the CDF can be interpreted as describing the \emph{probability} of
observing a value at most equal to $x$.\footnote{The distinction between
  likelihood and probability is important for continuous random variables;
  technically, any single value $x$ that a random variable $X$ might take has
  \emph{zero probability}, but a possibly-finite likelihood.}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.85\textwidth]{./images/dist_normal}

  \caption{Standard normal distribution PDF and CDF. Note that higher values of
    the PDF correspond to more likely values for the associated random variable;
    thus, this density's most likely value is zero. The CDF shows us that values
    below $Z\approx-2.5$ are highly unprobable.}
  \label{fig:dist-norm}
\end{figure}

In practice, we assume that data are generated by some underlying process, which
has some distribution -- we call this a \emph{population}. Data drawn from this
population are called a \emph{sample}. This distinction allows us to clarify the
difference between the sample summaries we considered above, and the
\emph{moments} we consider now.

\noindent\textbf{Moments} are properties of a distribution, and are directly
connected to the summaries introduced above. While sample moments summarize a
dataset, the moments we consider here summarize an entire function -- a
distribution. Generally, moments are quantities of the form $\E[X^k]$. Familiar
moments include the mean $\mu$ and variance $\sigma^2$, defined

\begin{equation} \begin{aligned}
    \mu      &\equiv \E[X], \\
    \sigma^2 &\equiv \E[(X-\E[X])^2], \\
             &= \E[X^2] - \E[X]^2,
\end{aligned} \end{equation}

\noindent These are exact moments for a given distribution; we may regard the
sample mean and sample variance introduced above as \emph{approximations} to
these true values.\footnote{Formally, assuming the existence of true moments or
  parameter values is a \emph{frequentist} philosophy, in contrast with a
  Bayesian perspective.} The difference between a sample moment computed from an
observed sample and the true moment is a form of \emph{sample error}, and is one
form of uncertainty we might seek to quantify. Assuming a distribution for an
underlying population will help us to do this, but keep in mind that our results
will only be as trustworthy as our assumptions!

Since moments are in effect summaries of distributions, two distributions may
share a few moments in common, and yet have a completely different shape. The
choice of data-modeling distribution is informed by this additional information
-- we will introduce a few important distributions, and provide brief commentary
on others.

\noindent\textbf{Uniform Distribution}

The uniform distribution is often used to encode constraints; if a value is
known (or asserted) to lie between some bounds, the uniform distribution puts
equal likelihood on every point between those bounds.

We denote a random variable drawn from a uniform distribution with bounds $a<b$
via

\begin{equation}
  U \sim U[a,b].
\end{equation}

\noindent\textbf{Normal Distribution}

We denote a random variable drawn from a normal distribution with mean $\mu$ and
variance $\sigma^2$ via

\begin{equation}
  X \sim N(\mu, \sigma^2).
\end{equation}

\noindent If $\mu=0$ and $\sigma^2=1$, then the associated normal is called
\emph{standard}, and is often denoted $Z \sim N(0, 1)$. Note that if we know the
true mean and variance, we can always standardize a normal random variable $X$
via

\begin{equation}
  \frac{X - \mu}{\sigma} \sim N(0, 1).
\end{equation}

The normal distribution is extremely common as an assumed distribution. This
assumption is endorsed by the central limit theorem, which gives the normal as
the limit distribution for sums of random variables, meeting certain technical
conditions.\cite{van1998asymptotic} For example, if we have a set of data $X_i$
for $i=1,\dots,n$ where each of the $X_i$ are drawn from the same population
with finite mean and variance, then as $n\to\infty$, we find

\begin{equation} \label{eq:mean-clt}
  \overline{X} = \frac{1}{n}\sum_{i=1}^n X_i \stackrel{d}{\to} N(\E[X], \V[X]/n),
\end{equation}

\noindent where $\stackrel{d}{\to}$ denotes convergence in distribution -- a
particular (weak) form of convergence. Note that while $X$ \emph{may not be
  normal}, the sample mean converges to a normal distribution centered around
the true mean $\E[X]$. Note also that the variance of this asymptotic
distribution scales as $1/n$; thus the standard deviation scales as
$1/\sqrt{n}$.

The normal distribution $N(\E[X], \V[X]/n)$ is called the \textbf{sampling
  distribution} for the estimator $\overline{X}$; this distribution will be
useful later when we define \emph{confidence intervals}. The standard deviation
of the sampling distribution -- in this case $\sigma/\sqrt{n}$ -- is called the
\textbf{standard error} of the estimate, and is a measure of the noise in our
estimate.

\noindent\textbf{Chi-Squared Distribution}

The sample mean is asymptotically normal under fairly generous conditions; can
we make a similar statement about the sample variance? The sample variance is
also a sum of random variables,\footnote{Recall $S^2 = \sum_{i=1}^n (X_i -
  \overline{X})^2$} but these may not be independent. \emph{However}, in the
case where $X$ is (exactly) normally distributed, the terms in the sample
variance \emph{are} independent. In this case, each term is an independent,
squared, standard normal random variable, which we denote

\begin{equation} \label{eq:chi-squared}
  \chi^2_{\nu} = \sum_{i=1}^{\nu} Z_i^2,
\end{equation}

\noindent and call a \emph{chi-squared} random variable with $\nu$ \emph{degrees
  of freedom}. One may show that

\begin{equation} \label{eq:sample-dist-var}
  S^2 \sim \sigma^2 \chi^2_{n-1} / (n-1),
\end{equation}

\noindent which is the sampling distribution for $S^2$, \emph{assuming} $X$ is
exactly normal. \textcolor{red}{TODO} Bessel's correction?

\noindent\textbf{Student's t Distribution}

The chi-squared distribution is constructed from squared normals. In the case
where $X$ is exactly normal, we may use the sample mean and variance to
construct a normalized quantity, which can be expressed in terms of the ratio of
an independent standard normal and chi-squared variables

\begin{equation} \label{eq:t-def}
  \frac{\overline{X} - \mu}{S/\sqrt{n}} = \frac{Z}{\sqrt{\chi^2_{n-1}/(n-1)}} \equiv t_{n-1},
\end{equation}

\noindent where $t_{n-1}$ follows \emph{Student's t distribution} with $n-1$
degrees of freedom. Contrast Student's t with the standardization of a
non-standard normal

\begin{equation}
  \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1).
\end{equation}

Student's t is important for \emph{hypothesis testing}; we introduce it here to
discuss the important concept of \emph{tail behavior}. The top panel of Figure
\ref{fig:dist-sheet} contrasts some example Student's t distributions against a
standard normal; note that -- in comparison with a gaussian -- Student's t
distribution is slower to decay to zero as one goes to $\pm\infty$. It is for
this reason that Student's t is said to have \emph{heavy} tails, compared to a
normal distribution.

Mosteller and Tukey\cite{mosteller1977data} discuss ways in which a distribution
can be nonnormal, and note that tail behavior tends to be both the hardest to
detect and the most consequential. Heavy tails imply that large values -- values
far from the center of the distribution -- tend to occur more frequently than we
might expect from a similar normal distribution. As Mosteller and Tukey note
this can ``alter a sample mean drastically, and a sample $S^2$
catastrophically!'' In practice, if a real process is suspected to exhibit heavy
tails, a t distribution may be a more appropriate model than a normal.

Figure \ref{fig:dist-sheet} depicts the distributions described above, for a
number of different parameters, and with their first two moments described in
terms of their parameters.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.85\textwidth]{./images/dist_sheet}

  \caption{Common densities and their first two moments. Note the heavy tails of
    the t distributions, and the shifting central tendency of the chi-squared
    distribution with increasing degrees of freedom.}
  \label{fig:dist-sheet}
\end{figure*}

\noindent\textbf{Other Distributions} \textcolor{red}{TODO} A list and brief
commentary on other distributions.

\noindent\textbf{Matching Moments} is a simple way to fit a distribution to
data. For a normal distribution this is easy, as it is parameterized in terms of
its mean and variance -- given data, we could easily compute the sample mean and
variance, and construct a normal distribution that matches these estimated
moments. This is not necessarily a good idea; suppose the moment estimates were
quite noisy (have high standard error) -- this additional variability would not
be accounted by simply matching moments.

These difficulties are compounded by the inherent difficulty in estimating
higher-order moments. While the normal distribution is defined by its mean and
variance alone, other distributions have more parameters to fit. In general,
higher-order moments -- larger exponents in $\E[X^k]$ -- are more difficult to
estimate. This is because we loose \textbf{degrees of freedom} in the data by
estimating the lower-moments. Recall that the sample mean $\overline{X}$ appears
in each term of the sample variance;\footnote{given by $S^2 = \sum_{i=1}^n (X_i
  - \overline{X})^2$} we have used the data to estimate the quantity
$\overline{X}$, and have lost some available information in doing so. This
notion of information is made formal in the degrees of freedom, which can be
understood in terms of the dimension of the subspace spanned by the data --
computing the sample mean essentially introduces a constraint, which reduces the
dimension of the data available for the variance by one. This reduction reduces
the effective sample size to $n-1$, which is captured in the Bessel correction,
and points to the difficulty in estimating the variance, relative to the mean.
Higher moments compound this difficulty with further reduction in degrees of
freedom.

The difficulty in estimating higher moments is closely related to tail behavior,
and is a fundamental issue. For instance, the kurtosis $\text{Kurt}[X] =
\E\left[\left(\frac{X - \mu}{\sigma}\right)^4\right]$ is a fourth-order moment,
commonly used to describe tail behavior -- a kurtosis greater (or smaller) than
that of a similar normal distribution points to heavier (or lighter) tails.

Finally, note that a chosen distribution introduces \emph{additional}
information beyond the fitted moments, in particular fixing higher-order moments
in terms of the parameters. For instance, assuming a normal vs Student's t makes
an assertion about tail behavior, and both assume a symmetric distribution.
Modeling a random quantity requires careful thought about what behavior it may
exhibit, and should ideally be supported by both data and relevant knowledge.

\noindent\textbf{Confidence Intervals} are a common way of expressing
uncertainty about an estimated quantity. In contrast with reporting a single
estimated value -- a \textbf{point estimate} -- a confidence interval brackets
the true quantity to be estimated, at a desired \emph{confidence level} -- a
probability of containing the true value. Note that the confidence in this
framework is in the \emph{procedure that generates the interval}, and \emph{not
  in the interval itself}.\footnote{Confidence intervals are frequentist
  constructions, which rely on the assumption that there exist true values to be
  estimated. Under this assumption, any given interval either does or does not
  include the true value. Thus, the associated confidence needs to be understood
  as a property of the process that generated the interval, and not a particular
  given interval itself.}

For example, it is common to seek confidence intervals for mean estimates, which
gives a sense of how trustworthy an estimate is -- a `wide' interval indicates
that one is not very certain about the estimate. Note that the sample mean
(under appropriate assumptions) is asymptotically normal;\footnote{with
  $\overline{X} \sim \dN(\mu, \sigma^2/n)$} one can use this limiting behavior
to construct \emph{approximate} confidence intervals. If we happened to have the
exact variance (which never happens in practice), we could define a symmetric
confidence interval around a given sample mean at a desired confidence level
$C\in[0,1]$ via

\begin{equation} \label{eq:mean-ci}
  [\overline{X} - z_C \sigma, \overline{X} + z_c \sigma],
\end{equation}

\noindent where $z_c = - \Phi^{-1}(\frac{1-C}{2})$, and $\Phi^{-1}(\cdot)$ is
the standard inverse normal CDF. In the case where $\overline{X}$ was exactly
normal, \Cref{eq:mean-ci} would have the desired coverage probability. However,
if $\overline{X}$ were only asymptotically normal this would not be the case,
but the realized coverage probability would approach the desired as one drew
more samples. Note that in practice, would would need to use the sample standard
deviation instead $S$, which would introduce another level of approximation. In
the case where $n$ is small, say $n < 10$, it may be prudent to account for this
additional approximation by using Student's t distribution.\footnote{One could
  do this by using a $t_c$ value in place of $z_c$, choosing from the inverse t
  CDF with appropriate degrees of freedom $df = n - 1$.}

\section{Models}
%% --------------------------------------------------
Scientists and engineers use models to answer questions. Scientists build models
to encode hypotheses relating physical quantities, then gather data to test
these hypotheses, in order to come to a fuller understanding of the physical
world. Engineers build predictive models to simulate physical processes, in
order to support the design of systems.

Above, we used distributions to model uncertain quantities; here, we will
discuss means to model \emph{relationships between quantities}. We will briefly
discuss different means to do this -- using either physics or data -- then focus
on how to assess models with regard to uncertainty, introducing the notion of
\emph{sensitivity analysis}, and detailing some methods and their usage.

To keep a consistent nomenclature, here and throughout we will consider models
which take some deterministic input $\vx \in \cD \subseteq \R{m}$ in a domain
$\cD$, and return a single, deterministic, scalar output $y \in \R{}$, with the
functional relation

\begin{equation}
  y = f(\vx).
\end{equation}

\noindent Note that we may have multiple output quantities of interest $y_i$,
which arise from multiple functions $f_i(\vx)$; however, not all techniques we
will study generalize well to vector outputs, so we will primarily consider
scalar outputs.

\subsection{Kinds of Models}
%% -------------------------
Scientists and engineers often derive \textbf{models from physics}; i.e. from
physical principles and laws. These relations are derived under particular
assumptions, and thus are only as accurate as their assumptions.

For instance, Newton's laws form a good model for the dynamics of rigid bodies
at human length- and time-scales. From this set of simple rules, one can derive
equations of motion which can be solved for the dynamics of a system.

Many physical laws involve constants that are fitted from physical data; for
instance, Newton's laws introduce mass as a constant of proportionality between
force and acceleration. The laws create a mapping from these (possibly fitted)
values to the output quantities of interest, such as the trajectory of a rigid
body. The physical laws will often include other inputs of interest, such as a
time duration ahead of some starting condition, or a location in space.

In the classical dynamics setting, the inputs $\vx$ might include the masses of
objects, initial positions, and a time of interest, while $y$ might be the
position of a particular body at the time of interest. These inputs, and their
dependent predictions, are usually thought to be determinstic under physical
laws.\footnote{Some notable exceptions include quantum mechanics and statistical
  mechanics.} For a single input $\vx$, we obtain a unique value $y$.

However, in many practical cases, the inputs $\vx$ are not known exactly, but
may be thought of instead as a multivariate random variable $\mX$. In this case,
the mapping $f(\vx)$ creates a new random variable $Y = f(\mX)$, with randomness
induced by the uncertain inputs $\mX$, and transformed by the model arising from
physics. This leads to a very practical question: ``Is the uncertainty in $Y$
significant?'' The answer to this question depends both on how uncertain we are
about $Y$, and for what purpose we intend to use this information. The latter
depends on problem formulation, while the former falls under the purview of
uncertainty propagation.

For instance, the gravitational constant $G$ is necessary for studying problems
in celestial mechanics. Some measurements of $G$ have uncertainty on the order
of 150 parts per million.\cite{rosi2014precision} Even without elaborate
uncertainty propagation, it seems reasonable that this level of uncertainty
would not pose a serious challenge to launching satellites to orbit. However,
the same measurement above disagrees with the accepted\footnote{by the Committee
  on Data for Science and Technology} value of $G$ by $1.5$ standard deviations
-- for scientific purposes this difference could be significant, particularly
since there is no predicted value for $G$ arising from theory.

Scientists and engineers also derive \textbf{models from data}, which share many
of the same characteristics as models from physics.

\subsection{Model Form Uncertainty}
%% -------------------------


\subsection{Sensitivity Analysis: Overview}
%% -------------------------
Above, we posed the question ``Is the uncertainty in $Y$ significant?'' A full
answer to this question requires choosing a context and computing the
uncertainty in $Y$. \textbf{Sensitivity analysis} asks a slightly different
question: ``How do uncertainties in $\mX$ affect those in $Y$?'' First studying
the sensitivity of a model $f(\cdot)$ can identify the inputs which are most
important to $Y$, which has multiple benefits.

For instance, one might be interested in reducing the uncertainty in $Y$, and
would like to determine a subset of $\mX$ for further study, in order to reduce
input uncertainty for a subset of variables. Sensitivity analysis can also allow
one to determine if particular inputs are \emph{unimportant}, with regard to the
output uncertainty. This enables one to fix these unimportant inputs to nominal
values, making uncertainty propagation cheaper, possibly rendering an
intractable computational problem feasible.

There are at least two broad families of sensitivity analysis: local and global.
Figure \ref{fig:local-vs-global} provides a cartoon example, intended to
illustrate the difference and need for these two separate, broad mentalities.

\begin{figure}[!ht]
  \centering\includegraphics[width=0.75\textwidth]{./images/local_vs_global}
  \caption{Cartoon functions to depict differences between local and global
    sensitivity. The vertical axis corresponds to the output qoi, while the
    horizontal corresponds to an input quantity. The black dot is the point of
    interest for local sensitivity. The blue curve has a great deal of global
    sensitivity, but zero local sensitivity. The red curve has comparatively
    little global sensitivity, but a large local sensitivity. Which metric is
    important depends on the objective of the study.}
  \label{fig:local-vs-global}
\end{figure}

\textbf{Local sensitivity analysis} usually refers to computing derivatives of
an output quantity $y$ with respect to the inputs $\vx$.\footnote{Sometimes
  `sensitivity' is used interchangeably with `derivative'.} A local analysis is
appropriate when one is interested in the behavior at and around `special'
points in the domain $\cD$. For example, we might be interested in the stability
of an optimal value for a constrained optimization problem -- a local
sensitivity analysis would aid in this study.\footnote{The Karush-Kuhn-Tucker
  conditions formalize some -- but not all -- aspects of this sort of
  sensitivity analysis.\footnote{boyd2004convex}}

\textbf{Global sensitivity analysis} usually refers to attributing
\emph{variability} in the output $y$ to the different inputs $\vx$. Often,
variability is quantified in terms of variance.\footnote{though alternatives
  exist; for instance one may seek to attribute portions of skewness, kurtosis,
  or other moments to different inputs.} For example, we might be interested in
determining which single input $X_i$ contributes the most variance to the output
$Y$, in order to better characterize the distribution of $X_i$ and ultimate
reduce uncertainty in the output.

As implied by the names, the two mentalities seek different pieces of
information. Depending on the context, one or the other may be the `right'
perspective to adopt. Figure \ref{fig:local-vs-global} provides a simple cartoon
illustration of two cases where local and global sensitivities will be in
disagreement.

\subsection{Local Sensitivity Analysis}
%% -------------------------
[Motivation]

Local sensitivity analysis can be accomplished in a rather straightforward
fashion via a \textbf{finite difference}. For a first-order approximation, one
simply chooses a base point $\vx$ and considers a finite step-size $\Delta x$
approximation to the (partial) derivative

\begin{equation}
  \frac{\partial f}{\partial x_i} \approx \frac{f(x_1,\dots,x_i+\Delta
    x,\dots,x_d) - f(\vx)}{\Delta x}.
\end{equation}

However, there are some complications with this approach. First, this
computational procedure requires additional evaluations, with a cost that grows
linearly with dimensionality $d$. If we require $N$ evaluations of the gradient
for a desired procedure (e.g. building a map of the gradient or performing
quadrature), then our total cost will be $O(Nd)$.

The other complication is choosing a sensible $\Delta x$. Recent work has
studied the selection of $\Delta x$ based on measuring the /empirical noise/ of
a function; that is, tailoring the step-size based on the observered variability
in a computed qoi arising from a computational procedure.\cite{more2012}


\subsection{Global Sensitivity Analysis}
%% -------------------------


\section{Questions}
%% --------------------------------------------------


\end{document}
